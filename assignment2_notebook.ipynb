{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313219f8",
   "metadata": {},
   "source": [
    "# Assignment 2 – CSCN 8020: Reinforcement Learning\n",
    "\n",
    "## Q-Learning and Deep Q-Network (DQN) on Taxi-v3\n",
    "\n",
    "### Overview\n",
    "This notebook implements and compares two reinforcement learning algorithms:\n",
    "- **Q-Learning**: A model-free, value-based learning algorithm\n",
    "- **Deep Q-Network (DQN)**: A neural network-based approach to Q-Learning using experience replay\n",
    "\n",
    "Both algorithms are trained on the **Taxi-v3** environment from OpenAI Gymnasium.\n",
    "\n",
    "### Environment Description\n",
    "The Taxi-v3 environment is a 5x5 grid where:\n",
    "- A taxi must pick up a passenger at a random location and drop them off at their destination\n",
    "- Actions: South, North, East, West, Pickup, Dropoff (6 total)\n",
    "- State space: 500 discrete states (taxi position, passenger location, destination)\n",
    "- Rewards: +20 for successful drop-off, -1 for each step taken, -10 for illegal pickup/dropoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1055ffd",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c972a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ gymnasium>=0.29.0 installed\n",
      "✓ numpy>=1.24.0 installed\n",
      "✓ matplotlib>=3.7.0 installed\n",
      "✓ torch>=2.0.0 installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'gymnasium>=0.29.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'torch>=2.0.0'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(f\"✓ {package} installed\")\n",
    "    except:\n",
    "        print(f\"✗ Failed to install {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74533f",
   "metadata": {},
   "source": [
    "## Part 1: Environment Utilities\n",
    "\n",
    "These utility functions help us load the Taxi-v3 environment and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d2c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_environment(render_mode=None):\n",
    "    \"\"\"Load and return the Taxi-v3 Gymnasium environment.\"\"\"\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=render_mode)\n",
    "    return env\n",
    "\n",
    "\n",
    "def print_env_info(env):\n",
    "    \"\"\"Print basic information about the environment.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Taxi-v3 Environment Info\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Action Space:       {env.action_space}\")\n",
    "    print(f\"Observation Space:  {env.observation_space}\")\n",
    "    print(f\"Number of States:   {env.observation_space.n}\")\n",
    "    print(f\"Number of Actions:  {env.action_space.n}\")\n",
    "    print()\n",
    "    action_meanings = {\n",
    "        0: \"Move South (down)\",\n",
    "        1: \"Move North (up)\",\n",
    "        2: \"Move East (right)\",\n",
    "        3: \"Move West (left)\",\n",
    "        4: \"Pickup passenger\",\n",
    "        5: \"Drop off passenger\",\n",
    "    }\n",
    "    print(\"Action Meanings:\")\n",
    "    for a, desc in action_meanings.items():\n",
    "        print(f\"  {a}: {desc}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def decode_state(state: int):\n",
    "    \"\"\"\n",
    "    Decode a state scalar into its components.\n",
    "    State encoding: ((taxi_row * 5 + taxi_col) * 5 + passenger_loc) * 4 + destination\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        Encoded state value (0 – 499)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with taxi_row, taxi_col, passenger_loc, destination\n",
    "    \"\"\"\n",
    "    destination = state % 4\n",
    "    state //= 4\n",
    "    passenger_loc = state % 5\n",
    "    state //= 5\n",
    "    taxi_col = state % 5\n",
    "    taxi_row = state // 5\n",
    "\n",
    "    passenger_map = {0: \"Red\", 1: \"Green\", 2: \"Yellow\", 3: \"Blue\", 4: \"In Taxi\"}\n",
    "    destination_map = {0: \"Red\", 1: \"Green\", 2: \"Yellow\", 3: \"Blue\"}\n",
    "\n",
    "    info = {\n",
    "        \"taxi_row\": taxi_row,\n",
    "        \"taxi_col\": taxi_col,\n",
    "        \"passenger_loc\": passenger_map[passenger_loc],\n",
    "        \"destination\": destination_map[destination],\n",
    "    }\n",
    "    return info\n",
    "\n",
    "\n",
    "def print_state_info(state: int):\n",
    "    \"\"\"Print a human-readable description of a state scalar.\"\"\"\n",
    "    info = decode_state(state)\n",
    "    print(f\"State {state}:\")\n",
    "    print(f\"  Taxi Position  : row={info['taxi_row']}, col={info['taxi_col']}\")\n",
    "    print(f\"  Passenger Loc  : {info['passenger_loc']}\")\n",
    "    print(f\"  Destination    : {info['destination']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97b9f6",
   "metadata": {},
   "source": [
    "### Test Environment Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5973f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Taxi-v3 Environment Info\n",
      "==================================================\n",
      "Action Space:       Discrete(6)\n",
      "Observation Space:  Discrete(500)\n",
      "Number of States:   500\n",
      "Number of Actions:  6\n",
      "\n",
      "Action Meanings:\n",
      "  0: Move South (down)\n",
      "  1: Move North (up)\n",
      "  2: Move East (right)\n",
      "  3: Move West (left)\n",
      "  4: Pickup passenger\n",
      "  5: Drop off passenger\n",
      "\n",
      "Initial observation: 473\n",
      "State 473:\n",
      "  Taxi Position  : row=4, col=3\n",
      "  Passenger Loc  : Blue\n",
      "  Destination    : Green\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the environment\n",
    "env = load_environment()\n",
    "print_env_info(env)\n",
    "obs, _ = env.reset()\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print_state_info(obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c55c35",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning Implementation\n",
    "\n",
    "### Algorithm Overview\n",
    "Q-Learning is an off-policy, value-based reinforcement learning algorithm. It learns the optimal action-value function (Q-function) that maps state-action pairs to expected future rewards.\n",
    "\n",
    "**Update Rule:**\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\gamma$ is the discount factor\n",
    "- $r$ is the immediate reward\n",
    "- $s'$ is the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c9cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def train_qlearning(\n",
    "    alpha: float = 0.1,\n",
    "    epsilon: float = 0.1,\n",
    "    gamma: float = 0.9,\n",
    "    n_episodes: int = 10_000,\n",
    "    max_steps: int = 200,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Q-Learning agent on Taxi-v3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.ndarray  – final Q-table (500 x 6)\n",
    "    metrics : dict  – episode returns, steps, and running averages\n",
    "    \"\"\"\n",
    "    env = load_environment()\n",
    "    n_states = env.observation_space.n   # 500\n",
    "    n_actions = env.action_space.n       # 6\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    episode_returns = []\n",
    "    episode_steps   = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # ε-greedy action selection\n",
    "            if rng.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = int(np.argmax(Q[obs]))\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update\n",
    "            best_next = np.max(Q[next_obs])\n",
    "            Q[obs, action] += alpha * (reward + gamma * best_next - Q[obs, action])\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_steps.append(step + 1)\n",
    "        \n",
    "        if (ep + 1) % 1000 == 0:\n",
    "            print(f\"Episode {ep + 1}/{n_episodes} completed\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Compute 100-episode rolling average\n",
    "    avg_returns = np.convolve(episode_returns, np.ones(100) / 100, mode=\"valid\")\n",
    "    avg_steps   = np.convolve(episode_steps,   np.ones(100) / 100, mode=\"valid\")\n",
    "\n",
    "    metrics = {\n",
    "        \"episode_returns\": episode_returns,\n",
    "        \"episode_steps\":   episode_steps,\n",
    "        \"avg_returns\":     avg_returns.tolist(),\n",
    "        \"avg_steps\":       avg_steps.tolist(),\n",
    "        \"total_episodes\":  n_episodes,\n",
    "    }\n",
    "    return Q, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06b01f",
   "metadata": {},
   "source": [
    "### Metrics and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54888b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict: dict, title_suffix: str, save_dir: str = \"plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for label, m in metrics_dict.items():\n",
    "        x = range(1, len(m[\"avg_returns\"]) + 1)\n",
    "        axes[0].plot(x, m[\"avg_returns\"], label=label)\n",
    "        axes[1].plot(x, m[\"avg_steps\"],   label=label)\n",
    "\n",
    "    axes[0].set_title(\"Average Return per Episode (100-ep window)\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Average Return\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].set_title(\"Average Steps per Episode (100-ep window)\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Average Steps\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(save_dir, f\"qlearning_{title_suffix}.png\")\n",
    "    plt.savefig(fname, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"  Saved plot → {fname}\")\n",
    "\n",
    "\n",
    "def summarise(label: str, metrics: dict):\n",
    "    print(f\"\\n[{label}]\")\n",
    "    print(f\"  Total episodes       : {metrics['total_episodes']}\")\n",
    "    print(f\"  Mean steps/episode   : {np.mean(metrics['episode_steps']):.2f}\")\n",
    "    print(f\"  Mean return/episode  : {np.mean(metrics['episode_returns']):.2f}\")\n",
    "    print(f\"  Final 100-ep return  : {metrics['avg_returns'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53750e",
   "metadata": {},
   "source": [
    "### Train Q-Learning Agent - Baseline\n",
    "\n",
    "Train with default hyperparameters: α=0.1, ε=0.1, γ=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360dbe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q-Learning: Baseline α=0.1, ε=0.1, γ=0.9 ===\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[Baseline Q-Learning]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 30.45\n",
      "  Mean return/episode  : -21.63\n",
      "  Final 100-ep return  : 1.68\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Q-Learning: Baseline α=0.1, ε=0.1, γ=0.9 ===\")\n",
    "Q_base, m_base = train_qlearning(alpha=0.1, epsilon=0.1, gamma=0.9, n_episodes=5_000)\n",
    "summarise(\"Baseline Q-Learning\", m_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f56b0",
   "metadata": {},
   "source": [
    "### Hyperparameter Sweep - Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e295dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q-Learning: Learning Rate Sweep ===\n",
      "\n",
      "Training α=0.01 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[α=0.01]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 126.80\n",
      "  Mean return/episode  : -160.15\n",
      "  Final 100-ep return  : -71.33\n",
      "\n",
      "Training α=0.05 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[α=0.05]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 44.26\n",
      "  Mean return/episode  : -41.39\n",
      "  Final 100-ep return  : 2.55\n",
      "\n",
      "Training α=0.1 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[α=0.1]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 30.35\n",
      "  Mean return/episode  : -21.41\n",
      "  Final 100-ep return  : 2.23\n",
      "\n",
      "Training α=0.2 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[α=0.2]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 23.24\n",
      "  Mean return/episode  : -11.16\n",
      "  Final 100-ep return  : 1.89\n",
      "  Saved plot → plots\\qlearning_learning_rate_sweep.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Q-Learning: Learning Rate Sweep ===\")\n",
    "lr_metrics = {}\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1, 0.2]:\n",
    "    label = f\"α={alpha}\"\n",
    "    print(f\"\\nTraining {label} …\")\n",
    "    _, m = train_qlearning(alpha=alpha, epsilon=0.1, gamma=0.9, n_episodes=5_000)\n",
    "    summarise(label, m)\n",
    "    lr_metrics[label] = m\n",
    "\n",
    "# Plot comparison\n",
    "plot_metrics(lr_metrics, \"learning_rate_sweep\", \"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eaf40",
   "metadata": {},
   "source": [
    "### Hyperparameter Sweep - Exploration Rate (ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c802f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q-Learning: Exploration Factor (ε) Sweep ===\n",
      "\n",
      "Training ε=0.05 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[ε=0.05]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 29.57\n",
      "  Mean return/episode  : -17.69\n",
      "  Final 100-ep return  : 4.80\n",
      "\n",
      "Training ε=0.1 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[ε=0.1]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 30.43\n",
      "  Mean return/episode  : -21.59\n",
      "  Final 100-ep return  : 2.38\n",
      "\n",
      "Training ε=0.2 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[ε=0.2]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 32.89\n",
      "  Mean return/episode  : -32.79\n",
      "  Final 100-ep return  : -5.41\n",
      "\n",
      "Training ε=0.3 …\n",
      "Episode 1000/5000 completed\n",
      "Episode 2000/5000 completed\n",
      "Episode 3000/5000 completed\n",
      "Episode 4000/5000 completed\n",
      "Episode 5000/5000 completed\n",
      "\n",
      "[ε=0.3]\n",
      "  Total episodes       : 5000\n",
      "  Mean steps/episode   : 36.10\n",
      "  Mean return/episode  : -47.65\n",
      "  Final 100-ep return  : -15.72\n",
      "  Saved plot → plots\\qlearning_exploration_sweep.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Q-Learning: Exploration Factor (ε) Sweep ===\")\n",
    "eps_metrics = {}\n",
    "\n",
    "for epsilon in [0.05, 0.1, 0.2, 0.3]:\n",
    "    label = f\"ε={epsilon}\"\n",
    "    print(f\"\\nTraining {label} …\")\n",
    "    _, m = train_qlearning(alpha=0.1, epsilon=epsilon, gamma=0.9, n_episodes=5_000)\n",
    "    summarise(label, m)\n",
    "    eps_metrics[label] = m\n",
    "\n",
    "# Plot comparison\n",
    "plot_metrics(eps_metrics, \"exploration_sweep\", \"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857b2a7",
   "metadata": {},
   "source": [
    "## Part 3: Deep Q-Network (DQN) Implementation\n",
    "\n",
    "### Algorithm Overview\n",
    "DQN uses a neural network to approximate the Q-function instead of storing a lookup table. Key improvements:\n",
    "- **Experience Replay**: Store transitions and sample random batches for training to break temporal correlations\n",
    "- **Target Network**: Use a separate network for computing target Q-values to improve stability\n",
    "- **One-hot Encoding**: Convert discrete states to binary vectors for neural network input\n",
    "\n",
    "**Loss Function:**\n",
    "$$L = (Q(s,a) - (r + \\gamma \\max_{a'} Q_{target}(s',a')))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f780970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Fully-connected Q-Network for discrete state/action spaces.\"\"\"\n",
    "    def __init__(self, n_states: int, n_actions: int, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing and sampling transitions.\"\"\"\n",
    "    def __init__(self, capacity: int = 10_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def one_hot(state: int, n_states: int) -> np.ndarray:\n",
    "    \"\"\"Convert discrete state to one-hot encoded vector.\"\"\"\n",
    "    v = np.zeros(n_states, dtype=np.float32)\n",
    "    v[state] = 1.0\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7b895",
   "metadata": {},
   "source": [
    "### DQN Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edca9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    alpha: float  = 1e-3,\n",
    "    epsilon_start: float = 1.0,\n",
    "    epsilon_end:   float = 0.05,\n",
    "    epsilon_decay: int   = 5_000,\n",
    "    gamma: float   = 0.9,\n",
    "    n_episodes: int = 2_000,\n",
    "    max_steps:  int = 200,\n",
    "    batch_size: int = 64,\n",
    "    target_update: int = 100,\n",
    "    buffer_capacity: int = 10_000,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Train a DQN agent on Taxi-v3 and return training metrics.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    env = load_environment()\n",
    "    n_states  = env.observation_space.n   # 500\n",
    "    n_actions = env.action_space.n        # 6\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    policy_net = DQN(n_states, n_actions).to(device)\n",
    "    target_net = DQN(n_states, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "    buffer    = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    episode_returns = []\n",
    "    episode_steps   = []\n",
    "    global_step     = 0\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy exploration with decay\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(\n",
    "                -global_step / epsilon_decay\n",
    "            )\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_t = torch.tensor(one_hot(obs, n_states),\n",
    "                                       dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action = int(policy_net(state_t).argmax(dim=1).item())\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            buffer.push(one_hot(obs, n_states), action, reward,\n",
    "                        one_hot(next_obs, n_states), float(done))\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            global_step  += 1\n",
    "\n",
    "            # Experience Replay: Sample and train on batch\n",
    "            if len(buffer) >= batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = buffer.sample(batch_size)\n",
    "\n",
    "                states_t      = torch.tensor(states_b,      dtype=torch.float32).to(device)\n",
    "                actions_t     = torch.tensor(actions_b,     dtype=torch.long).to(device)\n",
    "                rewards_t     = torch.tensor(rewards_b,     dtype=torch.float32).to(device)\n",
    "                next_states_t = torch.tensor(next_states_b, dtype=torch.float32).to(device)\n",
    "                dones_t       = torch.tensor(dones_b,       dtype=torch.float32).to(device)\n",
    "\n",
    "                current_q = policy_net(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    max_next_q = target_net(next_states_t).max(dim=1).values\n",
    "                    target_q   = rewards_t + gamma * max_next_q * (1 - dones_t)\n",
    "\n",
    "                loss = nn.functional.mse_loss(current_q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if global_step % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_returns.append(total_reward)\n",
    "        episode_steps.append(step + 1)\n",
    "        \n",
    "        if (ep + 1) % 500 == 0:\n",
    "            print(f\"Episode {ep + 1}/{n_episodes} completed\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    avg_returns = np.convolve(episode_returns, np.ones(100) / 100, mode=\"valid\")\n",
    "    avg_steps   = np.convolve(episode_steps,   np.ones(100) / 100, mode=\"valid\")\n",
    "\n",
    "    metrics = {\n",
    "        \"episode_returns\": episode_returns,\n",
    "        \"episode_steps\":   episode_steps,\n",
    "        \"avg_returns\":     avg_returns.tolist(),\n",
    "        \"avg_steps\":       avg_steps.tolist(),\n",
    "        \"total_episodes\":  n_episodes,\n",
    "    }\n",
    "\n",
    "    torch.save(policy_net.state_dict(), \"dqn_model.pth\")\n",
    "    print(\"\\nDQN model saved → dqn_model.pth\")\n",
    "    return policy_net, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f77e6d",
   "metadata": {},
   "source": [
    "### DQN Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7010dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dqn_metrics(metrics: dict, save_dir: str = \"plots\"):\n",
    "    \"\"\"Plot DQN training metrics.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    x = range(1, len(metrics[\"avg_returns\"]) + 1)\n",
    "    axes[0].plot(x, metrics[\"avg_returns\"], color=\"steelblue\")\n",
    "    axes[0].set_title(\"DQN – Average Return (100-ep window)\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Average Return\")\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(x, metrics[\"avg_steps\"], color=\"darkorange\")\n",
    "    axes[1].set_title(\"DQN – Average Steps (100-ep window)\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Average Steps\")\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(save_dir, \"dqn_training.png\")\n",
    "    plt.savefig(fname, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"  Saved plot → {fname}\")\n",
    "\n",
    "\n",
    "def summarise_dqn(metrics: dict):\n",
    "    \"\"\"Print DQN training summary.\"\"\"\n",
    "    print(\"\\n[DQN Results]\")\n",
    "    print(f\"  Total episodes       : {metrics['total_episodes']}\")\n",
    "    print(f\"  Mean steps/episode   : {np.mean(metrics['episode_steps']):.2f}\")\n",
    "    print(f\"  Mean return/episode  : {np.mean(metrics['episode_returns']):.2f}\")\n",
    "    print(f\"  Final 100-ep return  : {metrics['avg_returns'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518be6bc",
   "metadata": {},
   "source": [
    "### Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede7cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Deep Q-Network on Taxi-v3 ===\n",
      "Using device: cpu\n",
      "Episode 500/2000 completed\n",
      "Episode 1000/2000 completed\n",
      "Episode 1500/2000 completed\n",
      "Episode 2000/2000 completed\n",
      "\n",
      "DQN model saved → dqn_model.pth\n",
      "\n",
      "[DQN Results]\n",
      "  Total episodes       : 2000\n",
      "  Mean steps/episode   : 69.65\n",
      "  Mean return/episode  : -69.70\n",
      "  Final 100-ep return  : -0.23\n",
      "  Saved plot → plots\\dqn_training.png\n",
      "DQN metrics saved → dqn_metrics.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training Deep Q-Network on Taxi-v3 ===\")\n",
    "model, dqn_metrics = train_dqn(n_episodes=2_000)\n",
    "summarise_dqn(dqn_metrics)\n",
    "plot_dqn_metrics(dqn_metrics, \"plots\")\n",
    "\n",
    "# Save metrics\n",
    "with open(\"dqn_metrics.json\", \"w\") as f:\n",
    "    json.dump(dqn_metrics, f, indent=2)\n",
    "print(\"DQN metrics saved → dqn_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e33e1f",
   "metadata": {},
   "source": [
    "## Part 4: Comparison - Q-Learning vs DQN\n",
    "\n",
    "Compare the performance of Q-Learning and DQN approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cbe7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q-Learning vs DQN Comparison ===\n",
      "Comparison plot saved → plots\\ql_vs_dqn.png\n",
      "\n",
      "[Q-Learning Performance]\n",
      "  Final 100-ep return: 1.68\n",
      "  Mean steps/episode: 30.45\n",
      "\n",
      "[DQN Performance]\n",
      "  Final 100-ep return: -0.23\n",
      "  Mean steps/episode: 69.65\n"
     ]
    }
   ],
   "source": [
    "# Create comparison plot\n",
    "def plot_comparison(q_metrics, dqn_metrics, save_dir=\"plots\"):\n",
    "    \"\"\"Compare Q-Learning and DQN performance.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    x_q = range(1, len(q_metrics[\"avg_returns\"]) + 1)\n",
    "    x_dqn = range(1, len(dqn_metrics[\"avg_returns\"]) + 1)\n",
    "    \n",
    "    axes[0].plot(x_q, q_metrics[\"avg_returns\"], label=\"Q-Learning\", linewidth=2)\n",
    "    axes[0].plot(x_dqn, dqn_metrics[\"avg_returns\"], label=\"DQN\", linewidth=2)\n",
    "    axes[0].set_title(\"Average Return Comparison\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Average Return\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(x_q, q_metrics[\"avg_steps\"], label=\"Q-Learning\", linewidth=2)\n",
    "    axes[1].plot(x_dqn, dqn_metrics[\"avg_steps\"], label=\"DQN\", linewidth=2)\n",
    "    axes[1].set_title(\"Average Steps Comparison\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Average Steps\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(save_dir, \"ql_vs_dqn.png\")\n",
    "    plt.savefig(fname, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Comparison plot saved → {fname}\")\n",
    "\n",
    "# Note: Use the baseline Q-Learning metrics from earlier\n",
    "print(\"\\n=== Q-Learning vs DQN Comparison ===\")\n",
    "plot_comparison(m_base, dqn_metrics, \"plots\")\n",
    "\n",
    "print(\"\\n[Q-Learning Performance]\")\n",
    "print(f\"  Final 100-ep return: {m_base['avg_returns'][-1]:.2f}\")\n",
    "print(f\"  Mean steps/episode: {np.mean(m_base['episode_steps']):.2f}\")\n",
    "\n",
    "print(\"\\n[DQN Performance]\")\n",
    "print(f\"  Final 100-ep return: {dqn_metrics['avg_returns'][-1]:.2f}\")\n",
    "print(f\"  Mean steps/episode: {np.mean(dqn_metrics['episode_steps']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e8ea3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Q-Learning:**\n",
    "- Simple tabular approach storing Q-values for all state-action pairs\n",
    "- Converges relatively quickly on discrete, small state spaces\n",
    "- Low memory overhead for Taxi-v3 (500 states × 6 actions)\n",
    "- Hyperparameter sensitivity: α and ε significantly impact learning\n",
    "\n",
    "**Deep Q-Network (DQN):**\n",
    "- Neural network approximates Q-function for scalability\n",
    "- Experience replay and target networks stabilize learning\n",
    "- Handles high-dimensional state spaces better than Q-Learning\n",
    "- More complex but more generalizable to larger problems\n",
    "\n",
    "### Hyperparameter Analysis\n",
    "- **Learning Rate (α)**: Impacts convergence speed and stability\n",
    "- **Exploration Rate (ε)**: Balance between exploration and exploitation\n",
    "- **Discount Factor (γ)**: Controls importance of future rewards\n",
    "\n",
    "Both algorithms successfully learn to solve the Taxi-v3 problem. The choice between them depends on problem complexity and available computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
